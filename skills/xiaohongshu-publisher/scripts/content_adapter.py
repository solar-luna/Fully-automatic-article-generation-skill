#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦å†…å®¹é€‚é…å™¨
å°†å¾®ä¿¡å…¬ä¼—å·å†…å®¹è½¬æ¢ä¸ºå°çº¢ä¹¦æ ¼å¼
"""

import re
import html
from typing import Dict, List, Tuple


class XiaohongshuContentAdapter:
    """å°çº¢ä¹¦å†…å®¹é€‚é…å™¨"""

    # å°çº¢ä¹¦é™åˆ¶
    MAX_TITLE_CHARS = 20
    MAX_CONTENT_CHARS = 800  # åŒ…å«emoji
    SAFE_CONTENT_CHARS = 700  # ç•™å‡ºemojiç©ºé—´

    def __init__(self):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        self.emoji_list = [
            "ğŸ”¥", "ğŸ’¡", "âœ…", "âš¡", "ğŸš€", "ğŸ’ª", "ğŸ¯", "ğŸ“Œ", "âœ¨", "ğŸ’",
            "ğŸŒŸ", "ğŸ‘", "ğŸ¤”", "ğŸ˜", "ğŸ”§", "ğŸ“Š", "ğŸ“ˆ", "ğŸ’»", "ğŸ¨", "ğŸ”®"
        ]

    def adapt_title(self, title: str) -> str:
        """
        é€‚é…æ ‡é¢˜åˆ°å°çº¢ä¹¦æ ¼å¼ï¼ˆæœ€å¤š20å­—ï¼‰

        Args:
            title: åŸå§‹æ ‡é¢˜

        Returns:
            é€‚é…åçš„æ ‡é¢˜
        """
        # å»é™¤HTMLæ ‡ç­¾
        title = re.sub(r'<[^>]+>', '', title)
        title = html.unescape(title)
        title = title.strip()

        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(title) > self.MAX_TITLE_CHARS:
            title = title[:self.MAX_TITLE_CHARS - 1] + "â€¦"
            print(f"  âš  æ ‡é¢˜è¶…é•¿ï¼Œå·²æˆªæ–­åˆ° {self.MAX_TITLE_CHARS} å­—")

        return title

    def extract_key_points(self, content: str) -> List[str]:
        """
        ä»å†…å®¹ä¸­æå–å…³é”®è¦ç‚¹

        Args:
            content: HTMLå†…å®¹

        Returns:
            å…³é”®è¦ç‚¹åˆ—è¡¨
        """
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', ' ', content)
        text = html.unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()

        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]

        # æå–å¸¦æ•°å­—ã€ç‰¹æ®Šæ ‡è®°çš„å¥å­ï¼ˆé€šå¸¸æ˜¯è¦ç‚¹ï¼‰
        key_points = []
        patterns = [
            r'^\d+[ã€.]\s*(.+)',  # "1. xxx"
            r'^[â€¢Â·-]\s*(.+)',      # "â€¢ xxx"
            r'^ï¼ˆ.*?ï¼‰\s*(.+)',     # "ï¼ˆæ ¸å¿ƒï¼‰xxx"
        ]

        for sent in sentences:
            matched = False
            for pattern in patterns:
                m = re.match(pattern, sent)
                if m:
                    point = m.group(1).strip()
                    if len(point) > 5:
                        key_points.append(point)
                        matched = True
                        break
            if not matched and len(key_points) < 3:
                # å¦‚æœè¦ç‚¹ä¸å¤Ÿï¼Œå–ä¸€äº›è¾ƒé•¿çš„å¥å­
                if len(sent) > 10 and len(sent) < 50:
                    key_points.append(sent)

        return key_points[:5]  # æœ€å¤š5ä¸ªè¦ç‚¹

    def adapt_content(self, content: str, title: str = "") -> Dict[str, any]:
        """
        é€‚é…å†…å®¹åˆ°å°çº¢ä¹¦æ ¼å¼

        Args:
            content: HTMLå†…å®¹
            title: æ ‡é¢˜

        Returns:
            åŒ…å«é€‚é…åå†…å®¹çš„å­—å…¸
        """
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', ' ', content)
        text = html.unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()

        # æå–å…³é”®è¦ç‚¹
        key_points = self.extract_key_points(content)

        # æ„å»ºå°çº¢ä¹¦é£æ ¼å†…å®¹
        xhs_content_parts = []

        # 1. å¼•å…¥å¥ï¼ˆå¦‚æœæœ‰æ ‡é¢˜çš„è¯ï¼‰
        if title:
            xhs_content_parts.append(f"{title}\n")

        # 2. æ ¸å¿ƒè¦ç‚¹ï¼ˆå¸¦emojiï¼‰
        if key_points:
            xhs_content_parts.append("æ ¸å¿ƒäº®ç‚¹ï¼š")
            for i, point in enumerate(key_points[:4], 1):
                emoji = self.emoji_list[i % len(self.emoji_list)]
                # å‹ç¼©è¦ç‚¹é•¿åº¦
                if len(point) > 30:
                    point = point[:28] + ".."
                xhs_content_parts.append(f"{emoji} {point}")

        # 3. æ€»ç»“/è¡ŒåŠ¨å»ºè®®
        xhs_content_parts.append("\nğŸ’¬ æ€ä¹ˆæ ·ï¼Ÿä½ è§‰å¾—è¿™ä¸ªå¦‚ä½•ï¼Ÿ")

        # åˆå¹¶å†…å®¹
        xhs_content = "\n".join(xhs_content_parts)

        # æ£€æŸ¥é•¿åº¦
        total_chars = len(xhs_content)
        if total_chars > self.MAX_CONTENT_CHARS:
            # éœ€è¦å‹ç¼©
            print(f"  âš  å†…å®¹è¿‡é•¿ ({total_chars} å­—)ï¼Œæ­£åœ¨å‹ç¼©...")
            xhs_content = self._compress_content(xhs_content)
            print(f"  âœ“ å‹ç¼©å: {len(xhs_content)} å­—")
        else:
            print(f"  âœ“ å†…å®¹é•¿åº¦: {len(xhs_content)} å­—")

        return {
            "content": xhs_content,
            "original_length": len(text),
            "adapted_length": len(xhs_content),
            "key_points": key_points
        }

    def _compress_content(self, content: str) -> str:
        """
        å‹ç¼©å†…å®¹åˆ°å®‰å…¨é•¿åº¦

        Args:
            content: å†…å®¹

        Returns:
            å‹ç¼©åçš„å†…å®¹
        """
        lines = content.split("\n")
        compressed = []

        for line in lines:
            if not line.strip():
                continue

            # è·³è¿‡ä¸€äº›ä¸é‡è¦çš„è¡Œ
            if any(keyword in line for keyword in ["ğŸ’¬ æ€ä¹ˆæ ·", "æ€»ç»“", "è¦ç‚¹"]):
                if len("\n".join(compressed)) < self.SAFE_CONTENT_CHARS - 50:
                    compressed.append(line)
                continue

            # æˆªæ–­é•¿è¡Œ
            if len(line) > 40:
                line = line[:38] + ".."

            # æ£€æŸ¥æ€»é•¿åº¦
            current_length = len("\n".join(compressed)) + len(line) + 1
            if current_length > self.SAFE_CONTENT_CHARS:
                break

            compressed.append(line)

        result = "\n".join(compressed)

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        if len(result) > self.MAX_CONTENT_CHARS:
            result = result[:self.MAX_CONTENT_CHARS - 1] + "â€¦"

        return result

    def generate_tags(self, title: str, content: str) -> List[str]:
        """
        æ ¹æ®å†…å®¹ç”Ÿæˆè¯é¢˜æ ‡ç­¾

        Args:
            title: æ ‡é¢˜
            content: å†…å®¹

        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        # å¸¸è§AI/æŠ€æœ¯ç›¸å…³æ ‡ç­¾
        common_tags = {
            "AI": ["#AI", "#äººå·¥æ™ºèƒ½", "#AIGC"],
            "æ¨¡å‹": ["#å¤§æ¨¡å‹", "#AIæ¨¡å‹", "#LLM"],
            "å·¥å…·": ["#AIå·¥å…·", "#æ•ˆç‡å·¥å…·", "#ç”Ÿäº§åŠ›å·¥å…·"],
            "ç¼–ç¨‹": ["#ç¼–ç¨‹", "#ä»£ç ", "#å¼€å‘è€…"],
            "ChatGPT": ["#ChatGPT", "#OpenAI"],
            "Claude": ["#Claude", "#Anthropic"],
            "GPT": ["#GPT", "#OpenAI"],
            "Python": ["#Python", "#ç¼–ç¨‹"],
            "æ•°æ®": ["#æ•°æ®åˆ†æ", "#å¤§æ•°æ®"],
            "å­¦ä¹ ": ["#å­¦ä¹ ", "#çŸ¥è¯†åˆ†äº«"],
            "ç§‘æŠ€": ["#ç§‘æŠ€", "#é»‘ç§‘æŠ€"],
            "æ•™ç¨‹": ["#æ•™ç¨‹", "#å…¥é—¨æ•™ç¨‹"],
        }

        text = (title + " " + content).lower()
        found_tags = []

        # åŒ¹é…å…³é”®è¯
        for keyword, tags in common_tags.items():
            if keyword.lower() in text:
                found_tags.extend(tags)
                if len(found_tags) >= 5:
                    break

        # é»˜è®¤æ ‡ç­¾
        if not found_tags:
            found_tags = ["#AI", "#ç§‘æŠ€"]

        return found_tags[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾


def main():
    """æµ‹è¯•å‡½æ•°"""
    adapter = XiaohongshuContentAdapter()

    # æµ‹è¯•æ ‡é¢˜
    long_title = "OpenAIå‘å¸ƒå…¨æ–°o4æ¨¡å‹ï¼šæ¨ç†èƒ½åŠ›æå‡3å€ï¼Œæ•°å­¦å‡†ç¡®ç‡è¾¾åˆ°94.5%"
    print("åŸæ ‡é¢˜:", long_title)
    print("é€‚é…å:", adapter.adapt_title(long_title))
    print()

    # æµ‹è¯•å†…å®¹
    test_content = """
    <h1>OpenAI o4æ¨¡å‹é‡ç£…å‘å¸ƒ</h1>
    <p>OpenAIä»Šæ—¥æ­£å¼å‘å¸ƒäº†å¤‡å—æœŸå¾…çš„o4æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå®ç°äº†é‡å¤§çªç ´ã€‚</p>
    <p>ç»è¿‡å¤šé¡¹åŸºå‡†æµ‹è¯•ï¼Œo4åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸Šå‡†ç¡®ç‡è¾¾åˆ°94.5%ï¼Œç›¸æ¯”o3æå‡45%ã€‚</p>
    <p>æ ¸å¿ƒäº®ç‚¹ï¼š</p>
    <ul>
    <li>æ•°å­¦å‡†ç¡®ç‡94.5%ï¼Œæå‡45%</li>
    <li>ç¼–ç¨‹æ•ˆç‡æå‡3å€</li>
    <li>æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ï¼ˆ128Kâ†’200Kï¼‰</li>
    </ul>
    """

    result = adapter.adapt_content(test_content, "OpenAI o4å‘å¸ƒ")
    print("é€‚é…åå†…å®¹:")
    print(result["content"])
    print()
    print("æ ‡ç­¾:", adapter.generate_tags("OpenAI o4å‘å¸ƒ", test_content))


if __name__ == "__main__":
    main()
