#!/usr/bin/env python3
"""
é€‰é¢˜ç­–ç•¥è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
æ ¹æ®æ—¶é—´æ®µé€‰æ‹©æœ€ä½³é€‰é¢˜ï¼Œæ¯æ¬¡åªé€‰1ç¯‡
"""

import json
import os
import sys
import re
import time
import yaml
from datetime import datetime

# Add scripts directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from title_generator import get_time_slot_type, TIME_SLOT_CONTENT


# æ’é™¤å…³é”®è¯é»‘åå•
EXCLUDE_KEYWORDS = [
    # èèµ„ç±»
    'èèµ„', 'èèµ„è½®', 'Aè½®', 'Bè½®', 'Cè½®', 'IPO', 'ä¸Šå¸‚', 'ä¼°å€¼', 'å¸‚å€¼',
    'æŠ•èµ„', 'å‹Ÿèµ„', 'èµ„é‡‘', 'æŠ•èµ„äºº',
    # ä¼ä¸šåŠ¨æ€
    'è£å‘˜', 'äººäº‹å˜åŠ¨', 'é«˜ç®¡ç¦»èŒ', 'è´¢æŠ¥', 'å­£åº¦æ”¶å…¥', 'è¥æ”¶',
    # çº¯å•†ä¸šæ–°é—»
    'æˆ˜ç•¥åˆä½œ', 'æ”¶è´­', 'å¹¶è´­',
    # éAIç›¸å…³ï¼ˆæ–°å¢ï¼‰
    'dietary', 'diet', 'food', 'nutrition', 'health', 'vaccine', 'medical',
    'é¥®é£Ÿ', 'è†³é£Ÿ', 'é£Ÿå“', 'è¥å…»', 'å¥åº·', 'ç–«è‹—', 'åŒ»ç–—',
]

# AIç›¸å…³å…³é”®è¯ï¼ˆå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªï¼‰
AI_KEYWORDS = [
    # è‹±æ–‡
    'ai', 'artificial intelligence', 'machine learning', 'ml', 'deep learning',
    'neural network', 'llm', 'large language model', 'gpt', 'claude', 'chatgpt',
    'generative ai', 'gen ai', 'transformer', 'diffusion', 'stable diffusion',
    'openai', 'anthropic', 'deepmind', 'hugging face', 'langchain',
    'prompt', 'embedding', 'vector', 'rag', 'fine-tuning', 'training',
    'inference', 'model', 'algorithm', 'computer vision', 'nlp',
    'natural language processing', 'reinforcement learning', 'agent',
    # ä¸­æ–‡
    'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹',
    'ç”Ÿæˆå¼', 'AI', 'æ™ºèƒ½', 'ç®—æ³•', 'æ¨¡å‹è®­ç»ƒ', 'å¾®è°ƒ', 'æ¨ç†',
    'æç¤ºè¯', 'å‘é‡', 'åµŒå…¥', 'è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€', 'å¼ºåŒ–å­¦ä¹ ', 'æ™ºèƒ½ä½“',
]

# å†…å®¹ç±»å‹è¯†åˆ«è§„åˆ™
CONTENT_TYPE_RULES = {
    'new_tool': {
        'keywords': ['å‘å¸ƒ', 'æ¨å‡º', 'å¼€æº', 'æ–°å·¥å…·', 'æ–°æ¨¡å‹', 'new model', 'release', 'launch'],
        'sources': ['github.com', 'huggingface.co', 'openai.com', 'anthropic.com'],
        'priority': 'high'
    },
    'tutorial': {
        'keywords': ['æ•™ç¨‹', 'æŠ€å·§', 'å¦‚ä½•', 'å®æˆ˜', 'æŒ‡å—', 'å…¥é—¨', 'tutorial', 'guide', 'how to'],
        'sources': ['medium.com', 'dev.to'],
        'priority': 'medium'
    },
    'industry_news': {
        'keywords': ['æŠ¥å‘Š', 'ç ”ç©¶', 'åˆ†æ', 'è¶‹åŠ¿', 'report', 'research', 'analysis'],
        'sources': ['theverge.com', 'techcrunch.com', 'venturebeat.com'],
        'priority': 'low'
    }
}


def load_content_types():
    """åŠ è½½å†…å®¹ç±»å‹é…ç½®"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(os.path.dirname(script_dir), 'config', 'content_types.yaml')

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"å†…å®¹ç±»å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return {}


def load_hotspots(cache_path=None):
    """åŠ è½½çƒ­ç‚¹ç¼“å­˜"""
    if cache_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cache_dir = os.path.join(os.path.dirname(script_dir), 'cache')
        cache_path = os.path.join(cache_dir, 'hotspots.json')

    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"çƒ­ç‚¹ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
        print("è¯·å…ˆè¿è¡Œ fetch_hotspots.py è·å–çƒ­ç‚¹")
        sys.exit(1)


def is_excluded(topic):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤è¯¥è¯é¢˜"""
    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    combined = title + ' ' + summary

    # æ£€æŸ¥æ’é™¤å…³é”®è¯
    for keyword in EXCLUDE_KEYWORDS:
        if keyword.lower() in combined:
            return True

    return False


def is_ai_related(topic):
    """æ£€æŸ¥è¯é¢˜æ˜¯å¦ä¸AIç›¸å…³ï¼ˆå¿…é¡»åŒ…å«AIå…³é”®è¯ï¼‰"""
    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    url = topic['url'].lower()
    combined = title + ' ' + summary + ' ' + url

    # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªAIå…³é”®è¯
    for keyword in AI_KEYWORDS:
        if keyword.lower() in combined:
            return True

    return False


def score_topic_by_type(topic, target_type):
    """
    æ ¹æ®ç›®æ ‡ç±»å‹å¯¹è¯é¢˜è¿›è¡Œè¯„åˆ†

    Args:
        topic: è¯é¢˜å­—å…¸
        target_type: ç›®æ ‡å†…å®¹ç±»å‹ (new_tool/tutorial/industry_news)

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    url = topic['url'].lower()
    combined = title + ' ' + summary

    # 1. ç±»å‹åŒ¹é…åº¦ (30åˆ†)
    type_keywords = {
        'new_tool': ['å‘å¸ƒ', 'æ¨å‡º', 'å¼€æº', 'new', 'release', 'launch', 'announce'],
        'tutorial': ['æ•™ç¨‹', 'æŠ€å·§', 'å¦‚ä½•', 'å®æˆ˜', 'guide', 'tutorial', 'how to', 'step'],
        'industry_news': ['æŠ¥å‘Š', 'ç ”ç©¶', 'åˆ†æ', 'è¶‹åŠ¿', 'report', 'research', 'analysis']
    }

    for kw in type_keywords.get(target_type, []):
        if kw in combined:
            score += 30
            reasons.append(f"åŒ¹é…{target_type}ç±»å‹")
            break

    # 2. æŠ€æœ¯æ·±åº¦ (25åˆ†)
    if any(kw in combined for kw in ['ä»£ç ', 'code', 'api', 'ç¤ºä¾‹', 'example']):
        score += 25
        reasons.append("æœ‰æŠ€æœ¯å†…å®¹")
    elif any(kw in combined for kw in ['æŠ€æœ¯', 'technical', 'ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model']):
        score += 18
        reasons.append("æœ‰æŠ€æœ¯ç»†èŠ‚")

    # 3. æ–°é¢–æ€§ (20åˆ†)
    for kw in ['å‘å¸ƒ', 'æ¨å‡º', 'å¼€æº', 'new', 'release', 'launch']:
        if kw in title:
            score += 20
            reasons.append("æœ€æ–°å‘å¸ƒ")
            break

    # 4. åŸºç¡€å®ç”¨æ€§ (15åˆ†)
    if any(kw in combined for kw in ['åº”ç”¨', 'application', 'æ¡ˆä¾‹', 'use case', 'å¦‚ä½•', 'how']):
        score += 15
        reasons.append("æœ‰åº”ç”¨åœºæ™¯")
    elif any(kw in combined for kw in ['å·¥å…·', 'tool', 'æ¡†æ¶', 'framework', 'åº“', 'library']):
        score += 10
        reasons.append("å®ç”¨å·¥å…·")

    # 5. æ¥æºæƒå¨æ€§ (10åˆ†)
    authoritative_sources = [
        'openai.com', 'anthropic.com', 'deepmind.com', 'ai.meta.com',
        'huggingface.co', 'arxiv.org', 'github.com'
    ]
    for source in authoritative_sources:
        if source in url:
            score += 10
            reasons.append(f"æƒå¨æ¥æº")
            break

    # 6. å®ç”¨æ€§åŠ åˆ† (æ–°å¢ï¼Œæœ€å¤š+20åˆ†)
    practicality_score, prac_reasons = score_practicality(topic)
    score += practicality_score
    reasons.extend(prac_reasons)

    return score, reasons


def score_practicality(topic):
    """
    è¯„ä¼°è¯é¢˜çš„å®ç”¨æ€§ï¼ˆè¯»è€…èƒ½ç›´æ¥ç”¨ï¼‰

    Returns:
        (score, reasons) - åˆ†æ•°å’ŒåŸå› åˆ—è¡¨
    """
    score = 0
    reasons = []

    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    url = topic['url'].lower()
    combined = title + ' ' + summary

    # ä¼˜å…ˆï¼šå¼€æºå·¥å…·/ä»£ç åº“ï¼ˆ+15åˆ†ï¼‰
    if 'github.com' in url:
        score += 15
        reasons.append("å¼€æºå¯ç”¨")

        # GitHub starsåŠ åˆ†
        stars = topic.get('stars', 0)
        if stars > 1000:
            score += 5
            reasons.append(f"é«˜äººæ°”({stars}+ stars)")

    # ä¼˜å…ˆï¼šæœ‰æ•™ç¨‹/ç¤ºä¾‹ï¼ˆ+10åˆ†ï¼‰
    if any(kw in combined for kw in ['tutorial', 'guide', 'how to', 'æ•™ç¨‹', 'å®æˆ˜', 'ç¤ºä¾‹']):
        score += 10
        reasons.append("æœ‰æ•™ç¨‹")

    # ä¼˜å…ˆï¼šå…è´¹å·¥å…·ï¼ˆ+5åˆ†ï¼‰
    if any(kw in combined for kw in ['free', 'open source', 'å…è´¹', 'å¼€æº']):
        score += 5
        reasons.append("å…è´¹")

    # é™ä½ï¼šçº¯ç†è®º/ç ”ç©¶ï¼ˆ-10åˆ†ï¼‰
    if any(kw in combined for kw in ['research', 'paper', 'study', 'ç ”ç©¶', 'è®ºæ–‡']) and \
       not any(kw in combined for kw in ['code', 'implementation', 'å®ç°', 'ä»£ç ']):
        score -= 10
        reasons.append("çº¯ç†è®º(é™ä½)")

    # é™ä½ï¼šä¼ä¸šå†…éƒ¨å·¥å…·ï¼ˆ-15åˆ†ï¼‰
    if any(kw in combined for kw in ['internal', 'enterprise only', 'ä¼ä¸šå†…éƒ¨', 'ä»…é™ä¼ä¸š']):
        score -= 15
        reasons.append("ä¸å¯å…¬å¼€ä½¿ç”¨(é™ä½)")

    return score, reasons


def calculate_type_match_score(topic, target_type, content_types_config):
    """
    è®¡ç®—å†…å®¹ç±»å‹åŒ¹é…åº¦ï¼ˆ30åˆ†ï¼‰

    Args:
        topic: è¯é¢˜å­—å…¸
        target_type: ç›®æ ‡å†…å®¹ç±»å‹
        content_types_config: å†…å®¹ç±»å‹é…ç½®

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    combined = title + ' ' + summary

    # è·å–ç›®æ ‡ç±»å‹çš„å…³é”®è¯
    if target_type in content_types_config:
        keywords = content_types_config[target_type].get('keywords', [])

        # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
        matched_keywords = sum(1 for kw in keywords if kw.lower() in combined)

        if matched_keywords >= 3:
            score = 30
            reasons.append(f"é«˜åº¦åŒ¹é…{target_type}ç±»å‹({matched_keywords}ä¸ªå…³é”®è¯)")
        elif matched_keywords >= 2:
            score = 20
            reasons.append(f"åŒ¹é…{target_type}ç±»å‹({matched_keywords}ä¸ªå…³é”®è¯)")
        elif matched_keywords >= 1:
            score = 10
            reasons.append(f"éƒ¨åˆ†åŒ¹é…{target_type}ç±»å‹")
        else:
            # å³ä½¿æ²¡æœ‰åŒ¹é…å…³é”®è¯ï¼Œä¹Ÿç»™åŸºç¡€åˆ†ï¼ˆAIç›¸å…³å†…å®¹ï¼‰
            score = 8
            reasons.append(f"AIç›¸å…³å†…å®¹ï¼ˆåŸºç¡€åˆ†ï¼‰")

    return score, reasons


def calculate_timeliness_score(topic):
    """
    è®¡ç®—æ—¶æ•ˆæ€§åˆ†ï¼ˆ20åˆ†ï¼‰
    å‘å¸ƒæ—¶é—´è¶Šè¿‘åˆ†æ•°è¶Šé«˜

    Args:
        topic: è¯é¢˜å­—å…¸

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    published_date = topic.get('published_date')
    if not published_date:
        # æé«˜æ— æ—¶é—´æˆ³å†…å®¹çš„é»˜è®¤åˆ†ï¼ˆä»10åˆ†æé«˜åˆ°12åˆ†ï¼‰
        return 12, ["æ— å‘å¸ƒæ—¶é—´(é»˜è®¤12åˆ†)"]

    try:
        # è®¡ç®—è·ç¦»ç°åœ¨çš„å°æ—¶æ•°
        from datetime import datetime
        pub_time = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
        now = datetime.now(pub_time.tzinfo)
        hours_ago = (now - pub_time).total_seconds() / 3600

        if hours_ago <= 24:
            score = 20
            reasons.append("24å°æ—¶å†…å‘å¸ƒ")
        elif hours_ago <= 48:
            score = 15
            reasons.append("48å°æ—¶å†…å‘å¸ƒ")
        elif hours_ago <= 72:
            score = 10
            reasons.append("72å°æ—¶å†…å‘å¸ƒ")
        else:
            score = 5
            reasons.append("è¶…è¿‡72å°æ—¶")
    except:
        score = 12
        reasons.append("æ—¶é—´è§£æå¤±è´¥(é»˜è®¤12åˆ†)")

    return score, reasons


def calculate_quality_score(topic):
    """
    è®¡ç®—åŸºç¡€è´¨é‡åˆ†ï¼ˆ30åˆ†ï¼‰
    æ ‡é¢˜å®Œæ•´æ€§ã€æè¿°æ¸…æ™°åº¦ã€æ¥æºå¯é æ€§

    Args:
        topic: è¯é¢˜å­—å…¸

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    title = topic.get('title', '')
    summary = topic.get('summary', '')
    url = topic.get('url', '')

    # 1. æ ‡é¢˜å®Œæ•´æ€§ï¼ˆ10åˆ†ï¼‰
    if len(title) >= 20 and len(title) <= 200:
        score += 10
        reasons.append("æ ‡é¢˜é•¿åº¦é€‚ä¸­")
    elif len(title) >= 10:
        score += 5
        reasons.append("æ ‡é¢˜è¾ƒçŸ­")

    # 2. æè¿°æ¸…æ™°åº¦ï¼ˆ10åˆ†ï¼‰
    if len(summary) >= 50:
        score += 10
        reasons.append("æè¿°è¯¦ç»†")
    elif len(summary) >= 20:
        score += 5
        reasons.append("æè¿°ç®€çŸ­")

    # 3. æ¥æºå¯é æ€§ï¼ˆ10åˆ†ï¼‰
    authoritative_sources = [
        'openai.com', 'anthropic.com', 'deepmind.com', 'ai.meta.com',
        'huggingface.co', 'arxiv.org', 'github.com', 'microsoft.com',
        'google.com', 'nvidia.com'
    ]
    for source in authoritative_sources:
        if source in url.lower():
            score += 10
            reasons.append("æƒå¨æ¥æº")
            break
    else:
        score += 5
        reasons.append("ä¸€èˆ¬æ¥æº")

    return score, reasons


def calculate_audience_relevance_score(topic):
    """
    è®¡ç®—å—ä¼—ç›¸å…³æ€§ï¼ˆ20åˆ†ï¼‰
    ä¸ç›®æ ‡å—ä¼—ç—›ç‚¹çš„ç›¸å…³æ€§

    Args:
        topic: è¯é¢˜å­—å…¸

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    combined = title + ' ' + summary

    # å—ä¼—ç—›ç‚¹å…³é”®è¯
    pain_point_keywords = {
        'æ•ˆç‡': ['efficiency', 'productivity', 'fast', 'quick', 'æ•ˆç‡', 'å¿«é€Ÿ', 'æé€Ÿ'],
        'æˆæœ¬': ['free', 'cost', 'cheap', 'å…è´¹', 'ä½æˆæœ¬', 'çœé’±'],
        'æ˜“ç”¨': ['easy', 'simple', 'beginner', 'ç®€å•', 'å…¥é—¨', 'æ–°æ‰‹'],
        'å®ç”¨': ['practical', 'useful', 'application', 'å®ç”¨', 'åº”ç”¨', 'è½åœ°'],
    }

    matched_pain_points = []
    for pain_point, keywords in pain_point_keywords.items():
        if any(kw in combined for kw in keywords):
            matched_pain_points.append(pain_point)

    if len(matched_pain_points) >= 2:
        score = 20
        reasons.append(f"é«˜åº¦ç›¸å…³({','.join(matched_pain_points)})")
    elif len(matched_pain_points) >= 1:
        score = 15
        reasons.append(f"ç›¸å…³({','.join(matched_pain_points)})")
    else:
        score = 10
        reasons.append("ä¸€èˆ¬ç›¸å…³")

    return score, reasons


def score_with_time_slot_keywords(topic, time_slot_keywords):
    """
    æ ¹æ®æ—¶é—´æ®µå…³é”®è¯æ‰“åˆ†ï¼ˆæ–°å¢20åˆ†ç»´åº¦ï¼‰

    Args:
        topic: è¯é¢˜å­—å…¸
        time_slot_keywords: å½“å‰æ—¶é—´æ®µçš„å…³é”®è¯åˆ—è¡¨

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    title = topic['title'].lower()
    summary = topic.get('summary', '').lower()
    combined = title + ' ' + summary

    # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
    matched_count = sum(1 for kw in time_slot_keywords if kw.lower() in combined)

    if matched_count >= 5:
        score = 20
        reasons.append(f"é«˜åº¦åŒ¹é…æ—¶é—´æ®µ({matched_count}ä¸ªå…³é”®è¯)")
    elif matched_count >= 3:
        score = 15
        reasons.append(f"åŒ¹é…æ—¶é—´æ®µ({matched_count}ä¸ªå…³é”®è¯)")
    elif matched_count >= 1:
        score = 10
        reasons.append(f"éƒ¨åˆ†åŒ¹é…æ—¶é—´æ®µ")
    else:
        score = 0
        reasons.append("ä¸åŒ¹é…æ—¶é—´æ®µ")

    return score, reasons


def score_topic_enhanced(topic, target_type, content_types_config):
    """
    å¢å¼ºç‰ˆè¯é¢˜è¯„åˆ†ï¼ˆ4ç»´åº¦è¯„åˆ†ç³»ç»Ÿï¼‰

    è¯„åˆ†ç»´åº¦ï¼š
    1. åŸºç¡€è´¨é‡åˆ†ï¼ˆ30åˆ†ï¼‰ï¼šæ ‡é¢˜å®Œæ•´æ€§ã€æè¿°æ¸…æ™°åº¦ã€æ¥æºå¯é æ€§
    2. æ—¶æ•ˆæ€§åˆ†ï¼ˆ20åˆ†ï¼‰ï¼šå‘å¸ƒæ—¶é—´è¶Šè¿‘åˆ†æ•°è¶Šé«˜
    3. å†…å®¹ç±»å‹åŒ¹é…åº¦ï¼ˆ30åˆ†ï¼‰ï¼šä¸å½“å‰æ—¶é—´æ®µç›®æ ‡ç±»å‹çš„åŒ¹é…ç¨‹åº¦
    4. å—ä¼—ç›¸å…³æ€§ï¼ˆ20åˆ†ï¼‰ï¼šä¸ç›®æ ‡å—ä¼—ç—›ç‚¹çš„ç›¸å…³æ€§

    é¢å¤–åŠ åˆ†ï¼š
    - æƒå¨æ¥æºï¼ˆOpenAI/Anthropic/DeepMindç­‰ï¼‰ï¼š+5åˆ†

    Args:
        topic: è¯é¢˜å­—å…¸
        target_type: ç›®æ ‡å†…å®¹ç±»å‹
        content_types_config: å†…å®¹ç±»å‹é…ç½®

    Returns:
        (total_score, detailed_reasons)
    """
    total_score = 0
    all_reasons = []

    # 1. åŸºç¡€è´¨é‡åˆ†ï¼ˆ30åˆ†ï¼‰
    quality_score, quality_reasons = calculate_quality_score(topic)
    total_score += quality_score
    all_reasons.append(f"åŸºç¡€è´¨é‡: {quality_score}/30 - {', '.join(quality_reasons)}")

    # 2. æ—¶æ•ˆæ€§åˆ†ï¼ˆ20åˆ†ï¼‰
    time_score, time_reasons = calculate_timeliness_score(topic)
    total_score += time_score
    all_reasons.append(f"æ—¶æ•ˆæ€§: {time_score}/20 - {', '.join(time_reasons)}")

    # 3. å†…å®¹ç±»å‹åŒ¹é…åº¦ï¼ˆ30åˆ†ï¼‰
    type_score, type_reasons = calculate_type_match_score(topic, target_type, content_types_config)
    total_score += type_score
    all_reasons.append(f"ç±»å‹åŒ¹é…: {type_score}/30 - {', '.join(type_reasons)}")

    # 4. å—ä¼—ç›¸å…³æ€§ï¼ˆ20åˆ†ï¼‰
    audience_score, audience_reasons = calculate_audience_relevance_score(topic)
    total_score += audience_score
    all_reasons.append(f"å—ä¼—ç›¸å…³: {audience_score}/20 - {', '.join(audience_reasons)}")

    # é¢å¤–åŠ åˆ†ï¼šé¡¶çº§æƒå¨æ¥æºï¼ˆ+5åˆ†ï¼‰
    url = topic.get('url', '').lower()
    top_sources = ['openai.com', 'anthropic.com', 'deepmind.com', 'ai.meta.com']
    if any(source in url for source in top_sources):
        total_score += 5
        all_reasons.append("é¢å¤–åŠ åˆ†: +5 - é¡¶çº§æƒå¨æ¥æº")

    return total_score, all_reasons


def enforce_diversity(sorted_topics, history):
    """
    ç¡®ä¿å†…å®¹ç±»å‹å¤šæ ·åŒ–
    é¿å…è¿ç»­å¤šå¤©å‘å¸ƒåŒä¸€ç±»å‹å†…å®¹

    Args:
        sorted_topics: å·²æ’åºçš„è¯é¢˜åˆ—è¡¨
        history: å†å²å‘å¸ƒè®°å½•

    Returns:
        è°ƒæ•´åçš„è¯é¢˜åˆ—è¡¨
    """
    if not history or len(history) < 3:
        return sorted_topics

    # ç»Ÿè®¡æœ€è¿‘9æ¡è®°å½•çš„å†…å®¹ç±»å‹ï¼ˆå‡è®¾æ¯å¤©3ç¯‡ï¼‰
    recent_types = [r.get('content_type', '') for r in history[-9:]]

    # è®¡æ•°æ¯ç§ç±»å‹å‡ºç°æ¬¡æ•°
    type_counts = {}
    for t in recent_types:
        if t:
            type_counts[t] = type_counts.get(t, 0) + 1

    # å¦‚æœæŸä¸ªç±»å‹å‡ºç°>5æ¬¡ï¼Œé™ä½è¯¥ç±»å‹çš„ä¼˜å…ˆçº§
    adjusted = False
    for topic in sorted_topics:
        topic_type = topic.get('content_type', '')
        if type_counts.get(topic_type, 0) >= 5:
            topic['score'] -= 20
            adjusted = True
            print(f"âš–ï¸  '{topic_type}'æœ€è¿‘å‘å¸ƒè¿‡å¤šï¼ˆ{type_counts[topic_type]}æ¬¡ï¼‰ï¼Œé™ä½ä¼˜å…ˆçº§")

    if adjusted:
        # é‡æ–°æ’åº
        sorted_topics.sort(key=lambda x: x['score'], reverse=True)
        print("ğŸ“Š å·²è°ƒæ•´æ’åºä»¥å¢åŠ å†…å®¹å¤šæ ·æ€§")

    return sorted_topics


def extract_keywords(topic):
    """ä»è¯é¢˜ä¸­æå–å…³é”®è¯é›†åˆ"""
    text = (topic.get('title', '') + ' ' + topic.get('summary', '')).lower()

    # ç§»é™¤åœç”¨è¯
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were',
                 'çš„', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†æ˜¯', 'åœ¨', 'ä¸º', 'ç”¨'}

    # åˆ†è¯ï¼ˆç®€å•çš„ç©ºæ ¼åˆ†å‰² + æ ‡ç‚¹ç¬¦å·å¤„ç†ï¼‰
    words = re.findall(r'\b\w+\b', text)

    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    keywords = {w for w in words if len(w) > 2 and w not in stopwords}

    return keywords


def calculate_similarity(topic1, topic2):
    """
    è®¡ç®—ä¸¤ä¸ªè¯é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
    ä½¿ç”¨Jaccardç›¸ä¼¼åº¦ç®—æ³•
    """
    # 1. æå–å…³é”®è¯é›†åˆ
    keywords1 = extract_keywords(topic1)
    keywords2 = extract_keywords(topic2)

    # 2. Jaccardç›¸ä¼¼åº¦ = äº¤é›† / å¹¶é›†
    intersection = keywords1 & keywords2
    union = keywords1 | keywords2

    if not union:
        return 0.0

    similarity = len(intersection) / len(union)

    # 3. å¦‚æœURLå®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›1.0
    if topic1.get('url') == topic2.get('url'):
        return 1.0

    return similarity


def load_publish_history(hours=48):
    """
    åŠ è½½å‘å¸ƒå†å²è®°å½•

    Args:
        hours: åŠ è½½æœ€è¿‘Nå°æ—¶å†…çš„è®°å½•ï¼ˆé»˜è®¤48å°æ—¶ï¼‰

    Returns:
        å†å²è®°å½•åˆ—è¡¨
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cache_dir = os.path.join(os.path.dirname(script_dir), 'cache')
    history_file = os.path.join(cache_dir, 'publish_history.json')

    if not os.path.exists(history_file):
        print(f"ğŸ“ å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        return []

    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if 'records' not in data:
            print(f"âš ï¸  å†å²è®°å½•æ ¼å¼é”™è¯¯ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        # è¿‡æ»¤å‡ºæœ€è¿‘Nå°æ—¶å†…çš„è®°å½•
        cutoff = time.time() - (hours * 3600)
        recent_records = [
            r for r in data['records']
            if r.get('timestamp', 0) > cutoff
        ]

        print(f"ğŸ“Š åŠ è½½äº† {len(recent_records)} æ¡å†å²è®°å½•ï¼ˆæœ€è¿‘{hours}å°æ—¶ï¼‰")
        return recent_records

    except json.JSONDecodeError as e:
        print(f"âš ï¸  å†å²è®°å½•JSONè§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸  åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
        return []


def select_non_duplicate_topic(sorted_topics, history, similarity_threshold=0.7):
    """
    ä»æ’åºåçš„è¯é¢˜åˆ—è¡¨ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªä¸é‡å¤çš„è¯é¢˜

    Args:
        sorted_topics: æŒ‰è¯„åˆ†é™åºæ’åˆ—çš„è¯é¢˜åˆ—è¡¨
        history: æœ€è¿‘çš„å‘å¸ƒå†å²
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰

    Returns:
        é€‰ä¸­çš„è¯é¢˜ï¼Œå¦‚æœå…¨éƒ¨é‡å¤åˆ™è¿”å›None
    """
    print(f"\n=== å»é‡æ£€æŸ¥ ===")
    print(f"å€™é€‰è¯é¢˜æ•°: {len(sorted_topics)}")
    print(f"å†å²è®°å½•æ•°: {len(history)}")
    print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")

    if not history:
        print("âœ… æ— å†å²è®°å½•ï¼Œç›´æ¥é€‰æ‹©ç¬¬ä¸€ä¸ªè¯é¢˜")
        return sorted_topics[0] if sorted_topics else None

    for i, topic in enumerate(sorted_topics):
        # æ£€æŸ¥æ˜¯å¦ä¸å†å²è®°å½•é‡å¤
        is_duplicate = False
        max_similarity = 0.0
        duplicate_with = None

        for historical in history:
            similarity = calculate_similarity(topic, historical)

            if similarity > max_similarity:
                max_similarity = similarity
                duplicate_with = historical.get('title', 'Unknown')

            if similarity >= similarity_threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            print(f"âœ… é€‰ä¸­ç¬¬ {i+1} ä¸ªè¯é¢˜ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}ï¼‰")
            print(f"   æ ‡é¢˜: {topic['title'][:50]}...")
            return topic
        else:
            print(f"âš ï¸  ç¬¬ {i+1} ä¸ªè¯é¢˜é‡å¤")
            print(f"   æ ‡é¢˜: {topic['title'][:50]}...")
            print(f"   ä¸ '{duplicate_with[:30]}...' ç›¸ä¼¼åº¦: {max_similarity:.2f}")

    # æ‰€æœ‰è¯é¢˜éƒ½é‡å¤
    print(f"âŒ å‰ {len(sorted_topics)} ä¸ªè¯é¢˜å…¨éƒ¨é‡å¤ï¼Œæœ¬æ¬¡æ”¾å¼ƒå‘å¸ƒ")
    return None


def select_single_topic(hotspots):
    """
    é€‰æ‹©å•ä¸ªæœ€ä½³é€‰é¢˜

    æ ¹æ®å½“å‰æ—¶é—´æ®µç¡®å®šç›®æ ‡ç±»å‹ï¼Œç„¶åé€‰æ‹©æœ€åŒ¹é…çš„é«˜åˆ†è¯é¢˜
    """
    # åŠ è½½å†…å®¹ç±»å‹é…ç½®
    content_types_config = load_content_types()

    # è·å–å½“å‰æ—¶é—´æ®µå’Œç›®æ ‡ç±»å‹
    target_type, time_slot = get_time_slot_type()
    time_info = TIME_SLOT_CONTENT[time_slot]
    time_keywords = time_info.get('keywords', [])  # æ–°å¢ï¼šè·å–æ—¶é—´æ®µå…³é”®è¯

    print(f"\n=== æ—¶é—´æ®µç­–ç•¥ ===")
    print(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%H:%M')}")
    print(f"æ—¶æ®µ: {time_slot}")
    print(f"ç›®æ ‡ç±»å‹: {time_info['description']}")
    print(f"å†…å®¹é‡ç‚¹: {time_info['focus']}")
    print(f"ç›®æ ‡è¯»è€…: {time_info['target_audience']}")
    print(f"åŒ¹é…å…³é”®è¯æ•°: {len(time_keywords)}")

    # è¿‡æ»¤æ’é™¤çš„è¯é¢˜å’ŒéAIç›¸å…³è¯é¢˜
    filtered = []
    excluded_count = 0
    non_ai_count = 0
    for topic in hotspots:
        if is_excluded(topic):
            excluded_count += 1
            continue
        if not is_ai_related(topic):
            non_ai_count += 1
            continue
        filtered.append(topic)

    print(f"\nè¿‡æ»¤äº† {excluded_count} ä¸ªæ— ç”¨è¯é¢˜")
    print(f"è¿‡æ»¤äº† {non_ai_count} ä¸ªéAIç›¸å…³è¯é¢˜")
    print(f"å‰©ä½™å€™é€‰: {len(filtered)} ä¸ª")

    if not filtered:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯é¢˜")
        return None

    # è¯„åˆ†å¹¶æ’åºï¼ˆä½¿ç”¨å¢å¼ºç‰ˆ4ç»´åº¦è¯„åˆ†ç³»ç»Ÿ + æ—¶é—´æ®µå…³é”®è¯åŒ¹é…ï¼‰
    for topic in filtered:
        # åŸæœ‰çš„4ç»´åº¦è¯„åˆ†
        base_score, base_reasons = score_topic_enhanced(topic, target_type, content_types_config)

        # æ–°å¢ï¼šæ—¶é—´æ®µå…³é”®è¯åŒ¹é…ï¼ˆ20åˆ†ï¼‰
        time_score, time_reasons = score_with_time_slot_keywords(topic, time_keywords)

        topic['score'] = base_score + time_score
        topic['score_reasons'] = base_reasons + time_reasons
        topic['content_type'] = target_type

    # æŒ‰åˆ†æ•°æ’åº
    filtered.sort(key=lambda x: x['score'], reverse=True)

    # åŠ è½½å†å²è®°å½•å¹¶æ£€æŸ¥é‡å¤
    history = load_publish_history(hours=48)

    # å¼ºåˆ¶å¤šæ ·æ€§ï¼ˆå¦‚æœæŸç±»å‹æœ€è¿‘å‘å¸ƒè¿‡å¤šï¼Œé™ä½å…¶ä¼˜å…ˆçº§ï¼‰
    filtered = enforce_diversity(filtered, history)

    # é€‰æ‹©ç¬¬ä¸€ä¸ªä¸é‡å¤çš„è¯é¢˜ï¼ˆæ£€æŸ¥å‰10ä¸ªï¼‰
    selected = select_non_duplicate_topic(filtered[:10], history, similarity_threshold=0.7)

    if selected is None:
        print("\nâŒ æ‰€æœ‰å€™é€‰è¯é¢˜å‡ä¸è¿‘æœŸå‘å¸ƒé‡å¤ï¼Œæ”¾å¼ƒæœ¬æ¬¡å‘å¸ƒ")
        return None

    print(f"\n=== é€‰å®šé€‰é¢˜ ===")
    print(f"æ ‡é¢˜: {selected['title']}")
    print(f"æ¥æº: {selected['source']}")
    print(f"è¯„åˆ†: {selected['score']}")
    print(f"åŸå› : {', '.join(selected['score_reasons'])}")
    print(f"URL: {selected['url']}")

    return selected


def classify_topic(topic):
    """è¯†åˆ«è¯é¢˜ç±»å‹"""
    title = topic['title'].lower()
    url = topic['url'].lower()

    scores = {}
    for type_name, rules in CONTENT_TYPE_RULES.items():
        score = 0

        # å…³é”®è¯åŒ¹é…
        for kw in rules['keywords']:
            if kw.lower() in title:
                score += 1

        # æ¥æºåŒ¹é…
        for source in rules['sources']:
            if source in url:
                score += 2

        scores[type_name] = score

    # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
    if max(scores.values()) == 0:
        return 'industry_news'  # é»˜è®¤ç±»å‹

    return max(scores, key=scores.get)


def score_topic(topic):
    """å¯¹è¯é¢˜è¿›è¡Œå®ç”¨æ€§è¯„åˆ†"""
    score = 0
    reasons = []

    # 1. æŠ€æœ¯æ·±åº¦ (30åˆ†)
    summary = topic.get('summary', '').lower()
    title = topic['title'].lower()

    if any(kw in title + summary for kw in ['ä»£ç ', 'code', 'api', 'æ•™ç¨‹', 'tutorial']):
        score += 30
        reasons.append("åŒ…å«æŠ€æœ¯å†…å®¹")
    elif any(kw in title + summary for kw in ['æŠ€æœ¯', 'technical', 'ç®—æ³•', 'algorithm']):
        score += 20
        reasons.append("åŒ…å«æŠ€æœ¯ç»†èŠ‚")
    else:
        score += 10

    # 2. æ–°é¢–æ€§ (25åˆ†)
    found_new = False
    for kw in ['å‘å¸ƒ', 'æ¨å‡º', 'å¼€æº', 'new', 'release', 'launch']:
        if kw in title:
            score += 25
            reasons.append("æ–°å‘å¸ƒ/å¼€æº")
            found_new = True
            break

    if not found_new and any(kw in title for kw in ['æ›´æ–°', 'update', 'å‡çº§', 'upgrade']):
        score += 18
        reasons.append("é‡è¦æ›´æ–°")

    # 3. å®ç”¨æ€§ (25åˆ†)
    if any(kw in title + summary for kw in ['åº”ç”¨', 'application', 'æ¡ˆä¾‹', 'example', 'å¦‚ä½•', 'how to']):
        score += 25
        reasons.append("æœ‰åº”ç”¨æ¡ˆä¾‹")
    elif any(kw in title + summary for kw in ['å·¥å…·', 'tool', 'æ¡†æ¶', 'framework', 'åº“', 'library']):
        score += 20
        reasons.append("å®ç”¨å·¥å…·")

    # 4. çƒ­åº¦ (20åˆ†)
    if topic.get('type') == 'github':
        stars = topic.get('stars', 0)
        if stars > 1000:
            score += 20
            reasons.append(f"GitHub {stars}+ stars")
        elif stars > 100:
            score += 14
            reasons.append(f"GitHub {stars}+ stars")
    else:
        score += 10

    return score, reasons



def save_selected_topic(topic, output_path=None):
    """ä¿å­˜é€‰ä¸­çš„è¯é¢˜"""
    if topic is None:
        return None

    if output_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cache_dir = os.path.join(os.path.dirname(script_dir), 'cache')
        os.makedirs(cache_dir, exist_ok=True)
        output_path = os.path.join(cache_dir, 'selected_topic.json')

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(topic, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… é€‰é¢˜å·²ä¿å­˜: {output_path}")
    return output_path


if __name__ == '__main__':
    # åŠ è½½çƒ­ç‚¹
    hotspots = load_hotspots()

    # é€‰æ‹©å•ä¸ªé€‰é¢˜ï¼ˆæ ¹æ®æ—¶é—´æ®µè‡ªåŠ¨åˆ¤æ–­ç±»å‹ï¼‰
    selected = select_single_topic(hotspots)

    # ä¿å­˜
    if selected:
        save_selected_topic(selected)
